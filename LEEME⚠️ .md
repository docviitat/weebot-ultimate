Esta es una aplicaci√≥n web en Flask-Next.js y el modelo open-source de Ollama Qwen2.5 con 1.5 billones de par√°metros el cual se corre en el host local (localhost) con la CPU del dispositivo de c√≥mputo en el que se quiera correr esta aplicaci√≥n. LA APLICACI√ìN EST√Å DOCKERIZADA. Cre√© el bot relativamente desde cero, cre√© la interfaz del webchat y utilic√© el modelo Qwen2.5:1.5b como LLM que corre localmente, entren√© el modelo del sistema de recomendaciones desde el backend en Python y los resultados que comparte los inyecto como contexto al modelo LLM de Qwen2.5:1.5b, de esta manera TODA la aplicaci√≥n esta construida sobre recursos open-source *wink*.

RECOMENDACIONES:
 ‚ö†Ô∏è Tener una CPU de 8GB m√≠nimo para que las respuestas del modelo LLM de OLLAMA se produzcan sin tanta lentitud ya que la eficiencia y rapidez en ejecuci√≥n del modelo depende de la m√°quina del usuario que corre la aplicaci√≥n. REALMENTE RECUERDA QUE LA RAPIDEZ DEPENDE ENTERAMENTE DE TU M√ÅQUINA‚ùó ‚ùó ‚ùó ‚ùó

PRECAUCIONES LOL:
 ‚ö†Ô∏è El modelo LLM solo tien 1.5b de par√°metros, gpt-3.5-turbo tiene 175b as√≠ que si el modelo no da respuestas adecuadas, confusas o incluso fuera de contexto puede ser por esto as√≠ que utilice el modelo con precauci√≥n. Utilic√© este modelo porque es open-source y no necesita de un servidor de 500GB para correr.
 
## Para correr la aplicaci√≥n

1. Clonar este repositorio.
Clonar este repositorio.


2. Instalar [Docker](https://www.docker.com/products/docker-desktop) y [Docker Compose](https://docs.docker.com/compose/) en tu PC.

3. Antes de correr los contenedores, dentro del directorio del proyecto haz un "pull" del modelo    LLM de Qwen2.5:1.5b con este comando: üö® ollama pull qwen2.5:1.5b üö®. Esto "arrastrar√°" el modelo dentro de la imagen de Ollama Docker.

3. De nuevo, dentro del directorio del proyecto, ejecutar el comando:
üö® docker compose up -d --build üö®. Este comando deber√° crear los contenedores de los servicios del backend y frontend, adem√°s del contenedor del modelo LLM de Ollama.

4. Ingresar a [localhost:3000](http://localhost:3000) en su navegador web. El backend en el puerto [localhost:5000](http://localhost:5000) est√° configurado para conectarse autom√°ticamente a Ollama a trav√©s de la URL `http://ollama:11434`, que es accesible dentro de los contenedores de Docker. 

5. La aplicaci√≥n deber√≠a de funcionar en ese punto. üëå

üö®üö®üö®üö®üö® AL CORRER LA OTRA APLICACI√ìN RECUERDA ELIMINAR TODOS LOS CONTENEDORES PORQUE TODOS ESTAN CORRIENDO EN LOS MISMOS PUERTOS.